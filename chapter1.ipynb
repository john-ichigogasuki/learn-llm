{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'positive', 'score': 0.9990087151527405}\n"
     ]
    }
   ],
   "source": [
    "text_classification_pipeline = pipeline(\n",
    "    model = \"llm-book/bert-base-japanese-v3-marc_ja\"\n",
    ")\n",
    "positive_text = \"世界には言葉がわからなくても感動する言葉がある\"\n",
    "print(text_classification_pipeline(positive_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'negative', 'score': 0.9494393467903137}\n"
     ]
    }
   ],
   "source": [
    "negative_text = \"世界には言葉が出ないほどひどい言葉がある\"\n",
    "print(text_classification_pipeline(negative_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'entailment', 'score': 0.9964311122894287}\n"
     ]
    }
   ],
   "source": [
    "nli_pipeline = pipeline(model=\"llm-book/bert-base-japanese-v3-jnli\")\n",
    "text = \"二人の男性がジェット機を見ています\"\n",
    "entailment_text = \"ジェット機を見ている人が二人います\"\n",
    "\n",
    "print(nli_pipeline({\"text\": text, \"text_pair\": entailment_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLI: Natural Language Interface\n",
    "１つの文（前提文）に対して別の文（仮設文）が真(Entailment), 偽(Contradiction), 不明(Neutral)であるかを判定するものである。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'contradiction', 'score': 0.9743013381958008}\n"
     ]
    }
   ],
   "source": [
    "contradiction_text = \"2人の男性が飛んでいます\"\n",
    "print(nli_pipeline({\"text\": text, \"text_pair\": contradiction_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'neutral', 'score': 0.9579222202301025}\n"
     ]
    }
   ],
   "source": [
    "neutral_text = \"二人の男性が、白い飛行機を眺めています\"\n",
    "print(nli_pipeline({\"text\":text, \"text_pair\": neutral_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4963757991790771\n"
     ]
    }
   ],
   "source": [
    "text_sim_pipeline = pipeline(\n",
    "    model=\"llm-book/bert-base-japanese-v3-jsts\",\n",
    "    function_to_apply = \"none\",\n",
    ")\n",
    "text = \"川縁でサーフボードを持った人たちがいます\"\n",
    "sim_text = \"私はサーフィンが好きだ\"\n",
    "\n",
    "result = text_sim_pipeline({\"text\": text, \"text_pair\": sim_text})\n",
    "print(result[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052118077874183655\n"
     ]
    }
   ],
   "source": [
    "dissim_text = \"トイレの壁に黒いタオルがかけられています\"\n",
    "result = text_sim_pipeline({\"text\": text, \"text_pair\": dissim_text})\n",
    "print(result[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7431767582893372\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "sim_enc_pipeline = pipeline(\n",
    "    model = 'llm-book/bert-base-japanese-v3-unsup-simcse-jawiki',\n",
    "    task = 'feature-extraction',\n",
    ")\n",
    "\n",
    "text_emb = sim_enc_pipeline(text, return_tensors=True)[0][0]\n",
    "sim_emb = sim_enc_pipeline(sim_text, return_tensors=True)[0][0]\n",
    "\n",
    "sim_pair_score = cosine_similarity(text_emb, sim_emb, dim=0)\n",
    "print(sim_pair_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43792033195495605\n"
     ]
    }
   ],
   "source": [
    "dissim_emb = sim_enc_pipeline(dissim_text, return_tensors=True)[0][0]\n",
    "\n",
    "dissim_pair_score = cosine_similarity(text_emb, dissim_emb, dim=0)\n",
    "print(dissim_pair_score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': None,\n",
      "  'entity_group': '人名',\n",
      "  'score': 0.99823624,\n",
      "  'start': None,\n",
      "  'word': '大谷 翔平'},\n",
      " {'end': None,\n",
      "  'entity_group': '地名',\n",
      "  'score': 0.9986874,\n",
      "  'start': None,\n",
      "  'word': '岩手 県 水沢 市'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    model = 'llm-book/bert-base-japanese-v3-ner-wikipedia-dataset',\n",
    "    aggregation_strategy='simple',\n",
    ")\n",
    "\n",
    "text='大谷翔平は岩手県水沢市出身のプロ野球選手'\n",
    "pprint(ner_pipeline(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER -> Named Face Transformers　パイプラインを意味する\n",
    "    ・複数のトークンが同じエンティティグループ（例：地名・人名）に属する場合にそれらのトークンをどのように集約するかを指定するためのオプションである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今夜はNHKスペシャル世界を変えた男だな\n",
      "認知症の研究をしています。周りの目が怖いです【インタビュー】\n"
     ]
    }
   ],
   "source": [
    "text2text_pipeline = pipeline(\n",
    "    model = 'llm-book/t5-base-long-livedoor-news-corpus'\n",
    ")\n",
    "article = 'ついに始まった三連休。テレビを見ながら過ごしている人も多いのではないだろうか？今夜おすすめなのはなんと言ってもNHKスペシャル世界を変えた男だな'\n",
    "self_article = '僕の名前は古賀勇大、九州大学大学院に通っている修士1年生です。今は就職活動が忙しいので全然研究に着手できていません。研究は自然言語処理を用いて認知症に関連する研究を行っていきたいと考えていますが、データ集めに苦労していて何も手が進んでいません。周りの目が怖いです。'\n",
    "print(text2text_pipeline(article)[0]['generated_text'])\n",
    "print(text2text_pipeline(self_article)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁', '今日', 'は', '天気', 'が良い', 'の', 'で']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('abeja/gpt2-large-japanese')\n",
    "tokenizer.tokenize('今日は天気が良いので')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は天気が良いので</s>外でお弁当を食べました。 外で食べると 気持ちがいいですね。 今日は\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'abeja/gpt2-large-japanese'\n",
    ")\n",
    "\n",
    "inputs = tokenizer('今日は天気が良いので', return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=30,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "generate_text = tokenizer.decode(\n",
    "    outputs[0], slip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
